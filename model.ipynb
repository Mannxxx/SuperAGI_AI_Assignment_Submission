{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Task 1\n\nimport torch\nimport torch.nn as nn\n\n# The multi-head self-attention module\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n\n        # Linear projections\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n\n        # Output linear layer\n        self.out = nn.Linear(d_model, d_model)\n\n    def forward(self, q, k, v, mask):\n        bs = q.size(0)\n\n        # Linear projections\n        k = self.k_linear(k).view(bs, -1, self.n_heads, self.d_model // self.n_heads).transpose(1, 2)\n        q = self.q_linear(q).view(bs, -1, self.n_heads, self.d_model // self.n_heads).transpose(1, 2)\n        v = self.v_linear(v).view(bs, -1, self.n_heads, self.d_model // self.n_heads).transpose(1, 2)\n\n        # Scaled dot-product attention\n        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_model / self.n_heads).float())\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Apply attention mask and softmax\n        scores = nn.functional.softmax(scores, dim=-1)\n        attention = torch.matmul(scores, v)\n\n        # Concatenate and linear layer\n        concat_attention = attention.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n        output = self.out(concat_attention)\n\n        return output\n\n\n# The point-wise feed-forward module\nclass PositionwiseFeedforward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionwiseFeedforward, self).__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(0.1)\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.linear1(x))\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\n\n# Combine attention and feed forward to make transformer module called GPT2Layer\nclass GPT2Layer(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super(GPT2Layer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, n_heads)\n        self.feedforward = PositionwiseFeedforward(d_model, d_model * 4)\n\n    def forward(self, x, mask):\n        # Multi-head self-attention\n        attn_output = self.self_attn(x, x, x, mask)\n        x = x + attn_output\n\n        # Feedforward layer\n        ff_output = self.feedforward(x)\n        x = x + ff_output\n\n        return x\n\n# Using our custom transformer module, we can create the GPT2 model\nclass GPT2(nn.Module):\n    def __init__(self, d_model=768, n_heads=12, num_layers=12):\n        super(GPT2, self).__init__()\n        self.embedding = nn.Embedding(50257, d_model)\n        self.layers = nn.ModuleList([GPT2Layer(d_model, n_heads) for _ in range(num_layers)])\n\n    def forward(self, x, mask):\n        x = self.embedding(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-01T16:43:14.278678Z","iopub.execute_input":"2023-12-01T16:43:14.279146Z","iopub.status.idle":"2023-12-01T16:43:14.306899Z","shell.execute_reply.started":"2023-12-01T16:43:14.279111Z","shell.execute_reply":"2023-12-01T16:43:14.305647Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Task 2\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# For all 3 alterations the common pitfall is that the effectiveness may vary depending on the nature of the data,\n# and all increase time/memory requirements.\n\n# Rotary Positional Embedding doesn't significantly affect the model size although run time may increase, \n# as it involves additional computations without introducing new parameters. \n# It can ideally capture longer-term dependencies in sequences \n# (due to introducion of a rotation-invariant positional encoding)\nclass RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, d_model):\n        super(RotaryPositionalEmbedding, self).__init__()\n        self.d_model = d_model\n\n    def forward(self, x, freq=10000):\n        pos = torch.arange(0, x.size(1)).unsqueeze(0).type_as(x)\n        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * -(torch.log(torch.tensor(freq).float()) / self.d_model))\n        pos_embedding = torch.cat([torch.sin(pos * div_term), torch.cos(pos * div_term)], dim=-1)\n        return x + pos_embedding[:, :x.size(1)].detach()\n\n# Group Query Attention does increase the size of the model as it introduces additional parameters \n# and also increased runtime due to additional computations.\n# As it allows attending to groups of queries simultaneously, it can potentially improving its ability to capture diverse information.\nclass GroupQueryAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super(GroupQueryAttention, self).__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n\n        self.linear_q = nn.Linear(d_model, d_model)\n        self.linear_k = nn.Linear(d_model, d_model)\n        self.linear_v = nn.Linear(d_model, d_model)\n\n        self.linear_out = nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.shape[0]\n\n        Q = self.linear_q(query)\n        K = self.linear_k(key)\n        V = self.linear_v(value)\n\n        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n\n        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.d_model**0.5\n\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, float('-1e20'))\n\n        attention = F.softmax(energy, dim=-1)\n        x = torch.matmul(attention, V)\n\n        x = x.permute(0, 2, 1, 3).contiguous()\n        x = x.view(batch_size, -1, self.d_model)\n\n        x = self.linear_out(x)\n\n        return x\n\n# Sliding Window Attention increases size due to the modification in attention mechanisms.\n# As it allows the model to attend to a specified range, which can be useful for processing long sequences efficiently.\n# A major pitfall/ design consideration is that there can be loss of information outside the sliding window, \n# and the choice of window size is critical. Also, larger window sizes increase computational requirements.\nclass SlidingWindowAttention(nn.Module):\n    def __init__(self, d_model, n_heads, window_size):\n        super(SlidingWindowAttention, self).__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.window_size = window_size\n\n        self.linear_q = nn.Linear(d_model, d_model)\n        self.linear_k = nn.Linear(d_model, d_model)\n        self.linear_v = nn.Linear(d_model, d_model)\n\n        self.linear_out = nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.size()\n\n        Q = self.linear_q(query)\n        K = self.linear_k(key)\n        V = self.linear_v(value)\n\n        sliding_window_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=self.window_size).unsqueeze(0).unsqueeze(0)\n        sliding_window_mask = sliding_window_mask.to(device=query.device, dtype=query.dtype)\n\n        energy = torch.matmul(Q, K.permute(0, 2, 1)) / (self.d_model**0.5)\n\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, float('-1e20'))\n\n        attention = F.softmax(energy.masked_fill(sliding_window_mask == 0, float('-1e20')), dim=-1)\n        x = torch.matmul(attention, V)\n\n        x = self.linear_out(x)\n\n        return x\n\n    \n#create a new GPT2Layer_task2 (tranformer module) with support for the above 3 mechanisms\nclass GPT2Layer_task2(nn.Module):\n    def __init__(self, d_model, n_heads, hidden_dim, rotary_positional_embedding=False,\n                 group_query_attention=False, sliding_window_attention=False):\n        super(GPT2Layer, self).__init__()\n        self.self_attention = GroupQueryAttention(d_model, n_heads) if group_query_attention else MultiHeadAttention(d_model, n_heads)\n        self.feed_forward = FeedForward(d_model, hidden_dim)\n        self.layer_norm1 = nn.LayerNorm(d_model)\n        self.layer_norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n\n        if rotary_positional_embedding:\n            self.positional_embedding = RotaryPositionalEmbedding(d_model)\n        else:\n            self.positional_embedding = nn.Embedding(512, d_model)  # Fixed positional embeddings for simplicity\n\n        if sliding_window_attention:\n            self.attention = SlidingWindowAttention(d_model, n_heads, window_size=5)\n        else:\n            self.attention = self.self_attention\n\n    def forward(self, x, mask):\n        positions = torch.arange(0, x.size(1)).expand(x.size(0), x.size(1)).to(x.device)\n        x = self.token_embedding(x) + self.positional_embedding(positions)\n\n        mask = (x != 0).unsqueeze(1).unsqueeze(2)  # Padding mask\n\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        x = self.fc(x)\n        return x\n\n\n# Using GPT2Layer_task2, we can create the GPT2 model for task 2\nclass GPT2_task2(nn.Module):\n    def __init__(self, d_model=768, n_heads=12, num_layers=12):\n        super(GPT2, self).__init__()\n        self.embedding = nn.Embedding(50257, d_model)\n        self.layers = nn.ModuleList([GPT2Layer_task2(d_model, n_heads) for _ in range(num_layers)])\n\n    def forward(self, x, mask):\n        x = self.embedding(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# task 3\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\n\n# Function to create a model and optimizer\ndef create_model_optimizer(lr=5e-5):\n    model = GPT2()\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    return model, optimizer\n\n# Function to train the model on a single GPU\ndef train_single_gpu(model, optimizer, criterion, dataloader, device):\n    model.train()\n    model.to(device)\n\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n# Function to train the model using Distributed Data Parallel (DDP)\ndef train_ddp(model, optimizer, criterion, dataloader, device):\n    # Initialize distributed training\n    torch.distributed.init_process_group(backend='nccl')\n    local_rank = torch.distributed.get_rank()\n    torch.cuda.set_device(local_rank)\n    device = torch.device(\"cuda\", local_rank)\n\n    # Create model and optimizer on each GPU\n    model, optimizer = create_model_optimizer()\n    model = DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n\n    # Move data loader to GPU\n    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=dataloader.batch_size,\n                                             shuffle=True, num_workers=dataloader.num_workers,\n                                             pin_memory=True, sampler=torch.utils.data.distributed.DistributedSampler(dataloader.dataset))\n\n    # Training loop\n    model.train()\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n# Function to train the model using Fully Sharded Data Parallel (FSDP)\ndef train_fsdp(model, optimizer, criterion, dataloader, device):\n    # Initialize distributed training\n    torch.distributed.init_process_group(backend='nccl')\n    local_rank = torch.distributed.get_rank()\n    torch.cuda.set_device(local_rank)\n    device = torch.device(\"cuda\", local_rank)\n\n    # Create model and optimizer on each GPU\n    model, optimizer = create_model_optimizer()\n    model = FullyShardedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n\n    # Move data loader to GPU\n    dataloader = torch.utils.data.DataLoader(dataloader.dataset, batch_size=dataloader.batch_size,\n                                             shuffle=True, num_workers=dataloader.num_workers,\n                                             pin_memory=True, sampler=torch.utils.data.distributed.DistributedSampler(dataloader.dataset))\n\n    # Training loop\n    model.train()\n    for inputs, targets in dataloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n# Sample dataset and dataloader\nclass SampleDataset(torch.utils.data.Dataset):\n    def __init__(self, num_samples=100, seq_length=10):\n        self.data = torch.randint(50257, (num_samples, seq_length))\n        self.targets = torch.randint(50257, (num_samples, seq_length))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return self.data[index], self.targets[index]\n\ndataset = SampleDataset()\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n\n# Define loss criterion\ncriterion = nn.CrossEntropyLoss()\n\n# Choose the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Training on a single GPU\nmodel_single_gpu, optimizer_single_gpu = create_model_optimizer()\ntrain_single_gpu(model_single_gpu, optimizer_single_gpu, criterion, dataloader, device)\n\n# Training using Distributed Data Parallel (DDP)\nmodel_ddp, optimizer_ddp = create_model_optimizer()\ntrain_ddp(model_ddp, optimizer_ddp, criterion, dataloader, device)\n\n# Training using Fully Sharded Data Parallel (FSDP)\nmodel_fsdp, optimizer_fsdp = create_model_optimizer()\ntrain_fsdp(model_fsdp, optimizer_fsdp, criterion, dataloader, device)\n","metadata":{},"execution_count":null,"outputs":[]}]}